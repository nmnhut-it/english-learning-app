{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üéß One-Click Audio Generator (Dual Voice)\n\nGenerate audio for **Tr√≠ Nh·ªõ D·ªãu D√†ng** with your cloned voice.\n\n## üé§ Voice Setup (QUAN TR·ªåNG)\nƒê·ªÉ c√≥ ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t, b·∫°n c·∫ßn **2 file voice ri√™ng bi·ªát**:\n- `my-voice-vi.m4a` - Ch·ªâ n√≥i ti·∫øng Vi·ªát (30-60 gi√¢y)\n- `my-voice-en.m4a` - Ch·ªâ n√≥i ti·∫øng Anh (30-60 gi√¢y)\n\n### G·ª£i √Ω n·ªôi dung ƒë·ªÉ record:\n\n**Ti·∫øng Vi·ªát (my-voice-vi.m4a):**\n> M·ªói bu·ªïi s√°ng th·ª©c d·∫≠y, t√¥i th∆∞·ªùng pha m·ªôt t√°ch c√† ph√™ n√≥ng v√† ng·ªìi b√™n c·ª≠a s·ªï ng·∫Øm nh√¨n th√†nh ph·ªë. Nh·ªØng tia n·∫Øng ƒë·∫ßu ti√™n len l·ªèi qua k·∫Ω l√°, mang theo h∆°i ·∫•m d·ªãu d√†ng c·ªßa m·ªôt ng√†y m·ªõi. Cu·ªôc s·ªëng th·∫≠t ƒë·∫πp khi ta bi·∫øt tr√¢n tr·ªçng nh·ªØng ƒëi·ªÅu nh·ªè b√© xung quanh m√¨nh. ƒê√¥i khi, h·∫°nh ph√∫c kh√¥ng n·∫±m ·ªü nh·ªØng ƒëi·ªÅu xa x√¥i, m√† ch√≠nh l√† nh·ªØng kho·∫£nh kh·∫Øc b√¨nh y√™n nh∆∞ th·∫ø n√†y.\n\n**Ti·∫øng Anh (my-voice-en.m4a):**\n> Every morning when I wake up, I usually make myself a hot cup of coffee and sit by the window watching the city. The first rays of sunshine filter through the leaves, bringing the gentle warmth of a new day. Life is beautiful when we appreciate the small things around us. Sometimes, happiness is not found in distant places, but in peaceful moments like these.\n\n## ‚ö° Quick Start\n1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**\n2. Add `GITHUB_TOKEN` to Colab Secrets (üîë sidebar)\n3. Upload voice files to `the-lost-chapter/voices/`\n4. **Run All** (Ctrl+F9)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title üé§ Clone Voice Only (Step 1) { display-mode: \"form\" }\n#@markdown ### Clone voice from your audio files - NO audio generation yet\n\nGITHUB_USERNAME = \"nmnhut-it\" #@param {type:\"string\"}\nREPO_NAME = \"english-learning-app\" #@param {type:\"string\"}\nBRANCH = \"claude/audio-book-app-8dJZq\" #@param {type:\"string\"}\n\nimport subprocess, sys, os\n\n# ========== STEP 1: Install ==========\nprint(\"=\"*50)\nprint(\"üì¶ Installing dependencies...\")\nprint(\"=\"*50)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n                \"coqui-tts\", \"torchcodec\", \"soundfile\", \"pydub\"], check=True)\n\nimport torch, json, re, numpy as np, soundfile as sf\nfrom pathlib import Path\nfrom datetime import datetime\nfrom pydub import AudioSegment\nfrom google.colab import userdata, files\nfrom IPython.display import Audio, display, HTML\n\nprint(f\"‚úÖ Installed! GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'None'}\")\n\n# ========== STEP 2: Clone Repo ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üì• Cloning repository...\")\nprint(\"=\"*50)\n\ntry:\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\nexcept:\n    GITHUB_TOKEN = input(\"Enter GitHub token: \")\n\nREPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\nREPO_DIR = Path(f\"/content/{REPO_NAME}\")\n\nif REPO_DIR.exists():\n    os.chdir(REPO_DIR)\n    subprocess.run([\"git\", \"pull\", \"origin\", BRANCH], check=True)\nelse:\n    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"-b\", BRANCH, REPO_URL, str(REPO_DIR)], check=True)\n\nos.chdir(REPO_DIR)\nsubprocess.run([\"git\", \"config\", \"user.email\", \"colab@thelostchapter.app\"])\nsubprocess.run([\"git\", \"config\", \"user.name\", \"TheLostChapter CMS\"])\n\nVOICES_DIR = REPO_DIR / \"the-lost-chapter\" / \"voices\"\nVOICES_DIR.mkdir(parents=True, exist_ok=True)\n\n# Check for dual voice samples\nvi_samples = list(VOICES_DIR.glob('*-vi.*')) + list(VOICES_DIR.glob('*_vi.*'))\nen_samples = list(VOICES_DIR.glob('*-en.*')) + list(VOICES_DIR.glob('*_en.*'))\nother_samples = [f for f in (list(VOICES_DIR.glob('*.m4a')) + list(VOICES_DIR.glob('*.mp3')) + list(VOICES_DIR.glob('*.wav')))\n                 if f not in vi_samples and f not in en_samples]\n\nprint(f\"‚úÖ Repository ready!\")\nprint(f\"\\nüéµ Voice samples found:\")\nprint(f\"   VI: {[f.name for f in vi_samples] if vi_samples else '‚ùå MISSING - need my-voice-vi.m4a'}\")\nprint(f\"   EN: {[f.name for f in en_samples] if en_samples else '‚ùå MISSING - need my-voice-en.m4a'}\")\nif other_samples:\n    print(f\"   ‚ö†Ô∏è Mixed/Other: {[f.name for f in other_samples]} (should be split into VI/EN)\")\n\n# Check for existing profiles\nexisting_vi = list(VOICES_DIR.glob('*-vi.pt')) + list(VOICES_DIR.glob('*_vi.pt')) + [VOICES_DIR / 'default-vi.pt']\nexisting_en = list(VOICES_DIR.glob('*-en.pt')) + list(VOICES_DIR.glob('*_en.pt')) + [VOICES_DIR / 'default-en.pt']\nprint(f\"\\nüé§ Voice profiles (.pt):\")\nprint(f\"   VI: {[f.name for f in existing_vi if f.exists()] or '(not cloned yet)'}\")\nprint(f\"   EN: {[f.name for f in existing_en if f.exists()] or '(not cloned yet)'}\")\n\n# ========== STEP 3: Load Model ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üöÄ Loading viXTTS model...\")\nprint(\"=\"*50)\n\nfrom huggingface_hub import hf_hub_download\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\nfrom TTS.tts.layers.xtts import tokenizer as xtts_tokenizer\n\n# Patch for Vietnamese\n_orig_preprocess = xtts_tokenizer.VoiceBpeTokenizer.preprocess_text\ndef _patched(self, txt, lang):\n    if lang == \"vi\":\n        txt = txt.replace('\"', '')\n        txt = re.sub(r'\\s+', ' ', txt)\n        return txt.strip()\n    return _orig_preprocess(self, txt, lang)\nxtts_tokenizer.VoiceBpeTokenizer.preprocess_text = _patched\n\nMODEL_DIR = Path(\"/content/models/vixtts\")\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\nfor f in [\"config.json\", \"model.pth\", \"vocab.json\"]:\n    if not (MODEL_DIR / f).exists():\n        print(f\"  Downloading {f}...\")\n        hf_hub_download(repo_id=\"capleaf/viXTTS\", filename=f, local_dir=str(MODEL_DIR))\n    else:\n        print(f\"  ‚úì {f} (cached)\")\n\nconfig = XttsConfig()\nconfig.load_json(str(MODEL_DIR / \"config.json\"))\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(config, checkpoint_path=str(MODEL_DIR / \"model.pth\"),\n                      vocab_path=str(MODEL_DIR / \"vocab.json\"))\nmodel.cuda()\nprint(f\"‚úÖ Model loaded on GPU!\")\n\n# ========== STEP 4: Clone Voices ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üé§ Cloning voices from samples...\")\nprint(\"=\"*50)\n\ndef convert_to_wav(input_file, output_name=\"speaker\"):\n    \"\"\"Convert any audio format to wav\"\"\"\n    wav_path = f\"/content/{output_name}.wav\"\n    ext = Path(input_file).suffix.lower()\n    \n    if ext == '.m4a':\n        audio = AudioSegment.from_file(str(input_file), format='m4a')\n    elif ext == '.mp3':\n        audio = AudioSegment.from_mp3(str(input_file))\n    elif ext == '.wav':\n        audio = AudioSegment.from_wav(str(input_file))\n    else:\n        audio = AudioSegment.from_file(str(input_file))\n    \n    audio = audio.set_frame_rate(22050).set_channels(1)\n    audio.export(wav_path, format=\"wav\")\n    duration = len(audio) / 1000\n    print(f\"  ‚úì Converted {ext} ‚Üí wav ({duration:.1f}s)\")\n    return wav_path, duration\n\ndef clone_voice(sample_file, profile_name, lang_code):\n    \"\"\"Clone voice and save as .pt file\"\"\"\n    profile_file = VOICES_DIR / f\"{profile_name}.pt\"\n    \n    if profile_file.exists():\n        print(f\"‚úÖ {lang_code.upper()}: Profile exists ({profile_name}.pt) - skipping clone\")\n        data = torch.load(profile_file, weights_only=False)\n        return data[\"gpt_cond_latent\"].cuda(), data[\"speaker_embedding\"].cuda()\n    \n    print(f\"üß¨ {lang_code.upper()}: Cloning from {sample_file.name}...\")\n    wav_path, duration = convert_to_wav(sample_file, f\"speaker_{lang_code}\")\n    \n    if duration < 10:\n        print(f\"  ‚ö†Ô∏è Warning: Audio is only {duration:.1f}s. Recommend 30-60s for best quality.\")\n    \n    gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=wav_path)\n    \n    torch.save({\n        \"gpt_cond_latent\": gpt_cond_latent.cpu(),\n        \"speaker_embedding\": speaker_embedding.cpu(),\n        \"source\": sample_file.name,\n        \"language\": lang_code,\n        \"duration\": duration,\n        \"created\": datetime.now().isoformat(),\n        \"model\": \"viXTTS\"\n    }, profile_file)\n    \n    print(f\"  ‚úÖ Saved as {profile_name}.pt\")\n    return gpt_cond_latent.cuda(), speaker_embedding.cuda()\n\n# Clone Vietnamese voice\nvoice_vi = None\nif vi_samples:\n    voice_vi = clone_voice(vi_samples[0], \"default-vi\", \"vi\")\nelif other_samples:\n    print(f\"\\n‚ö†Ô∏è VI: Using mixed file {other_samples[0].name} (not recommended)\")\n    voice_vi = clone_voice(other_samples[0], \"default-vi\", \"vi\")\nelse:\n    print(\"\\n‚ùå VI: No sample found! Please upload my-voice-vi.m4a to voices/\")\n\n# Clone English voice\nvoice_en = None\nif en_samples:\n    voice_en = clone_voice(en_samples[0], \"default-en\", \"en\")\nelif other_samples:\n    print(f\"\\n‚ö†Ô∏è EN: Using mixed file {other_samples[0].name} (not recommended)\")\n    voice_en = clone_voice(other_samples[0], \"default-en\", \"en\")\nelse:\n    print(\"\\n‚ùå EN: No sample found! Please upload my-voice-en.m4a to voices/\")\n\n# ========== STEP 5: Push Voice Profiles ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üöÄ Saving voice profiles to GitHub...\")\nprint(\"=\"*50)\n\nos.chdir(REPO_DIR)\nsubprocess.run([\"git\", \"add\", \"the-lost-chapter/voices/\"])\n\nresult = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"])\nif result.returncode == 0:\n    print(\"‚ö† No new voice profiles to commit.\")\nelse:\n    subprocess.run([\"git\", \"commit\", \"-m\", \"Add cloned voice profiles (VI/EN)\"])\n    subprocess.run([\"git\", \"push\", \"origin\", BRANCH])\n    print(f\"‚úÖ Voice profiles pushed to GitHub!\")\n\n# ========== Summary ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üìä VOICE CLONING COMPLETE!\")\nprint(\"=\"*50)\n\nfor f in sorted(VOICES_DIR.glob(\"*.pt\")):\n    data = torch.load(f, weights_only=False)\n    print(f\"   üé§ {f.name}\")\n    print(f\"      Source: {data.get('source', 'unknown')}\")\n    print(f\"      Language: {data.get('language', 'unknown')}\")\n    print(f\"      Duration: {data.get('duration', '?')}s\")\n\nif not voice_vi or not voice_en:\n    print(\"\\n‚ö†Ô∏è MISSING VOICES! Upload these files to the-lost-chapter/voices/:\")\n    if not voice_vi:\n        print(\"   - my-voice-vi.m4a (30-60 gi√¢y ti·∫øng Vi·ªát)\")\n    if not voice_en:\n        print(\"   - my-voice-en.m4a (30-60 seconds English)\")\nelse:\n    print(\"\\n‚úÖ Ready to generate audio! Run the next cell.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title üì§ Upload Voice Samples (Optional) { display-mode: \"form\" }\n#@markdown ### Run this if you haven't uploaded voice files to the repo yet\n\nfrom google.colab import files\nimport shutil\n\nprint(\"üì§ Upload your voice samples:\")\nprint(\"   - my-voice-vi.m4a (30-60 gi√¢y ti·∫øng Vi·ªát)\")\nprint(\"   - my-voice-en.m4a (30-60 seconds English)\")\nprint()\n\nuploaded = files.upload()\n\nif uploaded:\n    VOICES_DIR = Path(f\"/content/{REPO_NAME}/the-lost-chapter/voices\")\n    VOICES_DIR.mkdir(parents=True, exist_ok=True)\n    \n    for filename, content in uploaded.items():\n        # Auto-rename if needed\n        if 'vi' in filename.lower() or 'viet' in filename.lower():\n            new_name = \"my-voice-vi\" + Path(filename).suffix\n        elif 'en' in filename.lower() or 'eng' in filename.lower():\n            new_name = \"my-voice-en\" + Path(filename).suffix\n        else:\n            new_name = filename\n        \n        dest = VOICES_DIR / new_name\n        shutil.copy(filename, dest)\n        print(f\"‚úÖ Saved: {dest}\")\n    \n    print(f\"\\nüìÅ Files in voices/:\")\n    for f in VOICES_DIR.iterdir():\n        print(f\"   - {f.name}\")\nelse:\n    print(\"‚ö† No files uploaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title üéµ Generate Audio (Step 2) { display-mode: \"form\" }\n#@markdown ### Generate audio for all chapters using cloned voices\n\nBOOK_ID = \"gentle-mind\" #@param {type:\"string\"}\nSKIP_EXISTING_AUDIO = True #@param {type:\"boolean\"}\nGENERATE_VI = True #@param {type:\"boolean\"}\nGENERATE_EN = True #@param {type:\"boolean\"}\n\n# ========== Load Voice Profiles ==========\nprint(\"=\"*50)\nprint(\"üé§ Loading voice profiles...\")\nprint(\"=\"*50)\n\nvi_profile = VOICES_DIR / \"default-vi.pt\"\nen_profile = VOICES_DIR / \"default-en.pt\"\n\nif GENERATE_VI:\n    if vi_profile.exists():\n        vi_data = torch.load(vi_profile, weights_only=False)\n        gpt_vi = vi_data[\"gpt_cond_latent\"].cuda()\n        spk_vi = vi_data[\"speaker_embedding\"].cuda()\n        print(f\"‚úÖ VI: Loaded from {vi_data.get('source', 'unknown')}\")\n    else:\n        print(\"‚ùå VI: No profile found! Run Step 1 first.\")\n        GENERATE_VI = False\n\nif GENERATE_EN:\n    if en_profile.exists():\n        en_data = torch.load(en_profile, weights_only=False)\n        gpt_en = en_data[\"gpt_cond_latent\"].cuda()\n        spk_en = en_data[\"speaker_embedding\"].cuda()\n        print(f\"‚úÖ EN: Loaded from {en_data.get('source', 'unknown')}\")\n    else:\n        print(\"‚ùå EN: No profile found! Run Step 1 first.\")\n        GENERATE_EN = False\n\nif not GENERATE_VI and not GENERATE_EN:\n    raise Exception(\"‚ùå No voice profiles available! Run Step 1 first.\")\n\n# ========== Setup Directories ==========\nCONTENT_DIR = REPO_DIR / \"the-lost-chapter\" / \"content\" / \"books\"\nBOOK_DIR = CONTENT_DIR / BOOK_ID\nAUDIO_DIR = BOOK_DIR / \"audio\"\nAUDIO_DIR.mkdir(parents=True, exist_ok=True)\n\n# ========== Text Extraction ==========\ndef extract_text(content, lang=\"vi\"):\n    \"\"\"Extract Vietnamese or English text from bilingual content\"\"\"\n    lines = []\n    for line in content.split('\\n'):\n        line = line.strip()\n        if line.startswith('*') and line.endswith('*'): \n            if lang == \"en\":\n                # Extract English from *italic* blocks\n                lines.append(line.strip('*').strip())\n            continue\n        if line in ['---', '']: continue\n        if line.startswith('#'):\n            clean = line.lstrip('#').strip()\n            if '|' in clean:\n                parts = clean.split('|')\n                clean = parts[0].strip() if lang == \"vi\" else parts[1].strip()\n            elif lang == \"en\":\n                continue  # Skip non-bilingual headers for English\n            if clean: lines.append(clean)\n            continue\n        \n        # Regular text\n        if '|' in line:\n            # Bilingual line: \"Vietnamese | English\"\n            parts = line.split('|')\n            text = parts[0].strip() if lang == \"vi\" else parts[1].strip() if len(parts) > 1 else \"\"\n        else:\n            # Only Vietnamese (no translation marker)\n            text = line if lang == \"vi\" else \"\"\n        \n        if text and not text.startswith('*'):\n            lines.append(text)\n    \n    return ' '.join(lines)\n\ndef generate_audio(text, output_path, lang, gpt_cond, speaker_emb, pause=0.5):\n    \"\"\"Generate audio for text\"\"\"\n    sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip() and len(s.strip()) > 3]\n    \n    if not sentences:\n        print(f\"  ‚ö† No sentences to generate\")\n        return 0\n    \n    all_audio, timestamps = [], []\n    silence = np.zeros(int(24000 * pause))\n    current_time = 0.0\n    \n    for i, sentence in enumerate(sentences):\n        display_text = sentence[:50] + \"...\" if len(sentence) > 50 else sentence\n        print(f\"  [{i+1}/{len(sentences)}] {display_text}\")\n        \n        out = model.inference(\n            sentence + \".\", \n            lang, \n            gpt_cond, \n            speaker_emb, \n            temperature=0.7\n        )\n        audio_data = out[\"wav\"]\n        \n        duration = len(audio_data) / 24000\n        timestamps.append({\n            \"start\": round(current_time, 2), \n            \"end\": round(current_time + duration, 2), \n            \"text\": sentence\n        })\n        current_time += duration + pause\n        \n        all_audio.extend([audio_data, silence])\n    \n    combined = np.concatenate(all_audio)\n    sf.write(str(output_path), combined, 24000)\n    \n    # Save timestamps\n    with open(output_path.with_suffix('.json'), 'w', encoding='utf-8') as f:\n        json.dump(timestamps, f, ensure_ascii=False, indent=2)\n    \n    return len(combined) / 24000\n\n# ========== Generate Audio ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üéµ Generating audio for all chapters...\")\nprint(\"=\"*50)\n\nwith open(BOOK_DIR / \"book.json\") as f:\n    book = json.load(f)\n\nprint(f\"\\nüìñ Book: {book['title']}\")\nprint(f\"üìë Chapters: {book['chapters']}\")\nprint(f\"üáªüá≥ Vietnamese: {'ON' if GENERATE_VI else 'OFF'}\")\nprint(f\"üá∫üá∏ English: {'ON' if GENERATE_EN else 'OFF'}\")\nprint(f\"‚è≠Ô∏è Skip existing: {'ON' if SKIP_EXISTING_AUDIO else 'OFF'}\\n\")\n\ngenerated, skipped = 0, 0\n\nfor chapter_id in book['chapters']:\n    chapter_file = BOOK_DIR / \"chapters\" / f\"{chapter_id}.json\"\n    with open(chapter_file) as f:\n        chapter = json.load(f)\n    \n    print(f\"\\n{'='*40}\")\n    print(f\"üìñ {chapter_id}: {chapter['title']}\")\n    print(f\"{'='*40}\")\n    \n    # Collect all markdown content\n    all_content = '\\n'.join([\n        s.get('content', '') \n        for s in chapter.get('sections', []) \n        if s.get('type') == 'markdown'\n    ])\n    \n    # Generate Vietnamese\n    if GENERATE_VI:\n        output_vi = AUDIO_DIR / f\"{chapter_id}-vi.wav\"\n        if SKIP_EXISTING_AUDIO and output_vi.exists():\n            print(f\"‚è≠Ô∏è {chapter_id}-vi.wav: exists, skipping...\")\n            skipped += 1\n        else:\n            vi_text = extract_text(all_content, \"vi\")\n            if vi_text.strip():\n                print(f\"\\nüáªüá≥ Generating Vietnamese audio...\")\n                duration = generate_audio(vi_text, output_vi, \"vi\", gpt_vi, spk_vi)\n                print(f\"  ‚úÖ {output_vi.name} ({duration:.1f}s)\")\n                generated += 1\n            else:\n                print(f\"  ‚ö† No Vietnamese text found\")\n    \n    # Generate English\n    if GENERATE_EN:\n        output_en = AUDIO_DIR / f\"{chapter_id}-en.wav\"\n        if SKIP_EXISTING_AUDIO and output_en.exists():\n            print(f\"‚è≠Ô∏è {chapter_id}-en.wav: exists, skipping...\")\n            skipped += 1\n        else:\n            en_text = extract_text(all_content, \"en\")\n            if en_text.strip():\n                print(f\"\\nüá∫üá∏ Generating English audio...\")\n                duration = generate_audio(en_text, output_en, \"en\", gpt_en, spk_en)\n                print(f\"  ‚úÖ {output_en.name} ({duration:.1f}s)\")\n                generated += 1\n            else:\n                print(f\"  ‚ö† No English text found\")\n\nprint(f\"\\n{'='*50}\")\nprint(f\"üìä Summary: {generated} generated, {skipped} skipped\")\nprint(f\"{'='*50}\")\n\n# ========== Push to GitHub ==========\nprint(\"\\n\" + \"=\"*50)\nprint(\"üöÄ Pushing audio to GitHub...\")\nprint(\"=\"*50)\n\nos.chdir(REPO_DIR)\nsubprocess.run([\"git\", \"add\", \"the-lost-chapter/\"])\n\nresult = subprocess.run([\"git\", \"diff\", \"--cached\", \"--quiet\"])\nif result.returncode == 0:\n    print(\"‚ö† No changes to commit.\")\nelse:\n    subprocess.run([\"git\", \"commit\", \"-m\", f\"Generate dual-language audio for {BOOK_ID}\"])\n    subprocess.run([\"git\", \"push\", \"origin\", BRANCH])\n    print(f\"‚úÖ Audio pushed to GitHub!\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üéâ ALL DONE!\")\nprint(\"=\"*50)\nfor f in sorted(AUDIO_DIR.glob(\"*.wav\")):\n    print(f\"   üîä {f.name} ({f.stat().st_size/1024/1024:.1f} MB)\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#@title üîä Preview Audio\nchapter = \"ch01\" #@param [\"ch01\", \"ch02\", \"ch03\"]\nlanguage = \"vi\" #@param [\"vi\", \"en\"]\n\naudio_file = AUDIO_DIR / f\"{chapter}-{language}.wav\"\nif audio_file.exists():\n    print(f\"üîä Playing: {audio_file.name}\")\n    display(Audio(str(audio_file)))\nelse:\n    print(f\"‚ùå Not found: {audio_file}\")\n    print(f\"Available files:\")\n    for f in sorted(AUDIO_DIR.glob(\"*.wav\")):\n        print(f\"   - {f.name}\")",
   "outputs": []
  }
 ]
}