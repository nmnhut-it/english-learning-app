{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è VieNeu-TTS Voice Cloning System\n",
    "\n",
    "Production-ready voice cloning notebook with automatic audio preprocessing.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Auto-detect M4A files in project\n",
    "- ‚úÖ Multi-format support (M4A/MP3/WAV)\n",
    "- ‚úÖ Smart audio preprocessing\n",
    "- ‚úÖ Extract optimal segments (5-10s)\n",
    "- ‚úÖ Clean, maintainable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ System Check & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "!pip install -q vieneu --extra-index-url https://pnnbao97.github.io/llama-cpp-python-v0.3.16/cpu/\n",
    "!pip install -q soundfile librosa pydub noisereduce\n",
    "!apt-get install -qq espeak-ng ffmpeg\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "from IPython.display import Audio, display, HTML\n",
    "from pydub import AudioSegment\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "class AudioConfig:\n",
    "    TARGET_SAMPLE_RATE = 24000\n",
    "    TARGET_DB = -20\n",
    "    NOISE_REDUCTION_STRENGTH = 0.7\n",
    "    MIN_SEGMENT_DURATION = 3\n",
    "    MAX_SEGMENT_DURATION = 10\n",
    "    FRAME_LENGTH_MS = 0.025\n",
    "    HOP_LENGTH_MS = 0.010\n",
    "    RMS_THRESHOLD_MULTIPLIER = 0.5\n",
    "    MAX_SEGMENTS_TO_SHOW = 5\n",
    "\n",
    "class Directories:\n",
    "    UPLOADS = \"uploads\"\n",
    "    PROCESSED = \"processed\"\n",
    "    OUTPUTS = \"outputs\"\n",
    "    PROJECT_VOICES = \"/content/voices\"  # Update this path if needed\n",
    "    \n",
    "    @classmethod\n",
    "    def setup(cls):\n",
    "        \"\"\"Create all required directories\"\"\"\n",
    "        for dir_path in [cls.UPLOADS, cls.PROCESSED, cls.OUTPUTS]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "Directories.setup()\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Audio Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConverter:\n",
    "    \"\"\"Handles audio format conversion\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_wav(input_path: str, output_path: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Convert audio file to WAV format\n",
    "        \n",
    "        Args:\n",
    "            input_path: Source audio file path\n",
    "            output_path: Target WAV file path (auto-generated if None)\n",
    "        \n",
    "        Returns:\n",
    "            Path to converted WAV file\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = f\"{Directories.PROCESSED}/converted.wav\"\n",
    "        \n",
    "        input_format = Path(input_path).suffix[1:]  # Remove dot\n",
    "        print(f\"üîÑ Converting {input_format.upper()} to WAV...\")\n",
    "        \n",
    "        audio = AudioSegment.from_file(str(input_path))\n",
    "        audio = audio.set_channels(1).set_frame_rate(AudioConfig.TARGET_SAMPLE_RATE)\n",
    "        audio.export(output_path, format=\"wav\")\n",
    "        \n",
    "        print(f\"‚úÖ Converted: {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioNormalizer:\n",
    "    \"\"\"Normalizes audio levels\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(audio_data: np.ndarray, target_db: float = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize audio to target dB level\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Audio samples as numpy array\n",
    "            target_db: Target dB level (uses config default if None)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized audio data\n",
    "        \"\"\"\n",
    "        if target_db is None:\n",
    "            target_db = AudioConfig.TARGET_DB\n",
    "        \n",
    "        rms = np.sqrt(np.mean(audio_data**2))\n",
    "        if rms > 0:\n",
    "            target_rms = 10 ** (target_db / 20)\n",
    "            audio_data = audio_data * (target_rms / rms)\n",
    "        \n",
    "        return np.clip(audio_data, -1, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_noise(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply noise reduction\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Audio samples\n",
    "            sample_rate: Sample rate in Hz\n",
    "        \n",
    "        Returns:\n",
    "            Noise-reduced audio\n",
    "        \"\"\"\n",
    "        print(\"üîá Applying noise reduction...\")\n",
    "        noise_sample = audio_data[:int(sample_rate * 0.5)]\n",
    "        return nr.reduce_noise(\n",
    "            y=audio_data,\n",
    "            sr=sample_rate,\n",
    "            y_noise=noise_sample,\n",
    "            prop_decrease=AudioConfig.NOISE_REDUCTION_STRENGTH\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechSegment:\n",
    "    \"\"\"Represents a speech segment with metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, start: float, end: float, energy: float):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.energy = energy\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.end - self.start\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SpeechSegment({self.start:.1f}s-{self.end:.1f}s, {self.duration:.1f}s)\"\n",
    "\n",
    "\n",
    "class SpeechSegmentDetector:\n",
    "    \"\"\"Detects speech segments in audio\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_segments(audio_data: np.ndarray, sample_rate: int) -> list[SpeechSegment]:\n",
    "        \"\"\"\n",
    "        Find speech segments using energy-based detection\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Audio samples\n",
    "            sample_rate: Sample rate\n",
    "        \n",
    "        Returns:\n",
    "            List of SpeechSegment objects, sorted by energy\n",
    "        \"\"\"\n",
    "        print(\"üîç Analyzing audio for speech segments...\")\n",
    "        \n",
    "        frame_length = int(sample_rate * AudioConfig.FRAME_LENGTH_MS)\n",
    "        hop_length = int(sample_rate * AudioConfig.HOP_LENGTH_MS)\n",
    "        \n",
    "        rms = librosa.feature.rms(\n",
    "            y=audio_data,\n",
    "            frame_length=frame_length,\n",
    "            hop_length=hop_length\n",
    "        )[0]\n",
    "        \n",
    "        threshold = np.mean(rms) * AudioConfig.RMS_THRESHOLD_MULTIPLIER\n",
    "        is_speech = rms > threshold\n",
    "        times = librosa.frames_to_time(\n",
    "            np.arange(len(rms)),\n",
    "            sr=sample_rate,\n",
    "            hop_length=hop_length\n",
    "        )\n",
    "        \n",
    "        segments = SpeechSegmentDetector._extract_segments(\n",
    "            times, is_speech, rms, audio_data, sample_rate\n",
    "        )\n",
    "        \n",
    "        if not segments:\n",
    "            segments = SpeechSegmentDetector._create_fixed_windows(\n",
    "                audio_data, sample_rate\n",
    "            )\n",
    "        \n",
    "        segments.sort(key=lambda x: x.energy, reverse=True)\n",
    "        return segments[:AudioConfig.MAX_SEGMENTS_TO_SHOW]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_segments(\n",
    "        times: np.ndarray,\n",
    "        is_speech: np.ndarray,\n",
    "        rms: np.ndarray,\n",
    "        audio_data: np.ndarray,\n",
    "        sample_rate: int\n",
    "    ) -> list[SpeechSegment]:\n",
    "        \"\"\"Extract segments from speech detection\"\"\"\n",
    "        segments = []\n",
    "        in_speech = False\n",
    "        start_time = 0\n",
    "        \n",
    "        for i, (t, speech) in enumerate(zip(times, is_speech)):\n",
    "            if speech and not in_speech:\n",
    "                start_time = t\n",
    "                in_speech = True\n",
    "            elif not speech and in_speech:\n",
    "                segment = SpeechSegmentDetector._create_segment(\n",
    "                    start_time, t, rms, i\n",
    "                )\n",
    "                if segment:\n",
    "                    segments.append(segment)\n",
    "                in_speech = False\n",
    "        \n",
    "        if in_speech:\n",
    "            segment = SpeechSegmentDetector._create_segment(\n",
    "                start_time, times[-1], rms, len(times)\n",
    "            )\n",
    "            if segment:\n",
    "                segments.append(segment)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_segment(\n",
    "        start: float,\n",
    "        end: float,\n",
    "        rms: np.ndarray,\n",
    "        index: int\n",
    "    ) -> SpeechSegment:\n",
    "        \"\"\"Create a segment if duration is valid\"\"\"\n",
    "        duration = end - start\n",
    "        if duration >= AudioConfig.MIN_SEGMENT_DURATION:\n",
    "            actual_end = min(end, start + AudioConfig.MAX_SEGMENT_DURATION)\n",
    "            actual_duration = actual_end - start\n",
    "            frame_count = int(actual_duration / AudioConfig.HOP_LENGTH_MS)\n",
    "            energy = np.mean(rms[max(0, index - frame_count):index])\n",
    "            return SpeechSegment(start, actual_end, energy)\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_fixed_windows(\n",
    "        audio_data: np.ndarray,\n",
    "        sample_rate: int\n",
    "    ) -> list[SpeechSegment]:\n",
    "        \"\"\"Create fixed-size windows as fallback\"\"\"\n",
    "        print(\"‚ö†Ô∏è No clear speech segments found, creating fixed windows...\")\n",
    "        segments = []\n",
    "        total_duration = len(audio_data) / sample_rate\n",
    "        window_step = 5\n",
    "        \n",
    "        for start in np.arange(\n",
    "            0,\n",
    "            max(1, total_duration - AudioConfig.MIN_SEGMENT_DURATION),\n",
    "            window_step\n",
    "        ):\n",
    "            end = min(start + AudioConfig.MAX_SEGMENT_DURATION, total_duration)\n",
    "            if end - start >= AudioConfig.MIN_SEGMENT_DURATION:\n",
    "                seg_data = audio_data[int(start * sample_rate):int(end * sample_rate)]\n",
    "                energy = np.sqrt(np.mean(seg_data**2))\n",
    "                segments.append(SpeechSegment(start, end, energy))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_segment_audio(\n",
    "        audio_data: np.ndarray,\n",
    "        sample_rate: int,\n",
    "        segment: SpeechSegment\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Extract audio data for a segment\"\"\"\n",
    "        start_sample = int(segment.start * sample_rate)\n",
    "        end_sample = int(segment.end * sample_rate)\n",
    "        return audio_data[start_sample:end_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    \"\"\"Main audio preprocessing pipeline\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(\n",
    "        input_path: str,\n",
    "        apply_noise_reduction: bool = True\n",
    "    ) -> tuple[np.ndarray, int, list[SpeechSegment]]:\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input audio file\n",
    "            apply_noise_reduction: Whether to apply noise reduction\n",
    "        \n",
    "        Returns:\n",
    "            (audio_data, sample_rate, segments)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üéõÔ∏è AUDIO PREPROCESSING PIPELINE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Convert format if needed\n",
    "        input_path = Path(input_path)\n",
    "        if input_path.suffix.lower() != '.wav':\n",
    "            wav_path = AudioConverter.convert_to_wav(str(input_path))\n",
    "        else:\n",
    "            wav_path = str(input_path)\n",
    "        \n",
    "        # Load audio\n",
    "        print(\"\\nüìÇ Loading audio...\")\n",
    "        audio_data, sample_rate = librosa.load(\n",
    "            wav_path,\n",
    "            sr=AudioConfig.TARGET_SAMPLE_RATE,\n",
    "            mono=True\n",
    "        )\n",
    "        print(f\"   Duration: {len(audio_data)/sample_rate:.1f}s\")\n",
    "        print(f\"   Sample rate: {sample_rate} Hz\")\n",
    "        \n",
    "        # Apply noise reduction\n",
    "        if apply_noise_reduction:\n",
    "            audio_data = AudioNormalizer.reduce_noise(audio_data, sample_rate)\n",
    "        \n",
    "        # Normalize\n",
    "        print(\"üìä Normalizing audio levels...\")\n",
    "        audio_data = AudioNormalizer.normalize(audio_data)\n",
    "        \n",
    "        # Find segments\n",
    "        segments = SpeechSegmentDetector.find_segments(audio_data, sample_rate)\n",
    "        print(f\"\\n‚úÖ Found {len(segments)} candidate segments\")\n",
    "        \n",
    "        return audio_data, sample_rate, segments\n",
    "\n",
    "print(\"‚úÖ Audio processing functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Find Available Voice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class VoiceFileFinder:\n    \"\"\"Finds available voice files in the project\"\"\"\n    \n    SUPPORTED_FORMATS = ['.m4a', '.mp3', '.wav']\n    \n    @classmethod\n    def find_all(cls, search_dirs: list[str] = None) -> list[Path]:\n        \"\"\"\n        Search for voice files in specified directories\n        \n        Args:\n            search_dirs: List of directories to search (uses defaults if None)\n        \n        Returns:\n            List of Path objects for found voice files\n        \"\"\"\n        if search_dirs is None:\n            # Search Colab environment directories\n            search_dirs = [\n                \"/content\",  # Colab content directory\n                \"./uploads\",  # Uploaded files directory\n                \".\",  # Current directory\n            ]\n        \n        found_files = []\n        for search_dir in search_dirs:\n            if os.path.exists(search_dir):\n                try:\n                    for fmt in cls.SUPPORTED_FORMATS:\n                        found_files.extend(Path(search_dir).rglob(f\"*{fmt}\"))\n                except (PermissionError, OSError):\n                    # Skip directories we can't access\n                    continue\n        \n        return sorted(set(found_files))\n    \n    @classmethod\n    def display_found_files(cls) -> list[Path]:\n        \"\"\"Find and display all voice files\"\"\"\n        print(\"üîç Searching for voice files in Colab environment...\\n\")\n        files = cls.find_all()\n        \n        if files:\n            print(f\"‚úÖ Found {len(files)} voice file(s):\\n\")\n            for i, file_path in enumerate(files, 1):\n                size_mb = file_path.stat().st_size / (1024 * 1024)\n                print(f\"{i}. {file_path}\")\n                print(f\"   Size: {size_mb:.2f} MB\")\n                print()\n        else:\n            print(\"‚ùå No voice files found in Colab environment.\")\n            print(\"üí° You'll be prompted to upload your M4A/MP3/WAV file next.\")\n        \n        return files\n\n# Search for existing voice files\navailable_files = VoiceFileFinder.display_found_files()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load TTS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vieneu import Vieneu\n",
    "\n",
    "class TTSModelInfo:\n",
    "    \"\"\"TTS model information and capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.has_clone_voice = hasattr(model, 'clone_voice')\n",
    "        self.has_encode_reference = hasattr(model, 'encode_reference')\n",
    "        self.has_create_voice = hasattr(model, 'create_voice')\n",
    "    \n",
    "    def display_capabilities(self):\n",
    "        \"\"\"Display model capabilities\"\"\"\n",
    "        print(\"üîç Model capabilities:\")\n",
    "        print(f\"   clone_voice: {'‚úÖ' if self.has_clone_voice else '‚ùå'}\")\n",
    "        print(f\"   encode_reference: {'‚úÖ' if self.has_encode_reference else '‚ùå'}\")\n",
    "        print(f\"   create_voice: {'‚úÖ' if self.has_create_voice else '‚ùå'}\")\n",
    "    \n",
    "    def list_all_methods(self):\n",
    "        \"\"\"List all available methods\"\"\"\n",
    "        methods = [m for m in dir(self.model) if not m.startswith('_')]\n",
    "        print(\"\\nüìã All available methods:\")\n",
    "        for method in methods:\n",
    "            print(f\"   - {method}\")\n",
    "    \n",
    "    def get_preset_voices(self) -> list[str]:\n",
    "        \"\"\"Get list of preset voice names\"\"\"\n",
    "        try:\n",
    "            voices = self.model.list_preset_voices()\n",
    "            if isinstance(voices, list) and voices:\n",
    "                if isinstance(voices[0], tuple):\n",
    "                    return [name for _, name in voices]\n",
    "                return voices\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not list voices: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load model\n",
    "print(\"üîÑ Loading VieNeu-TTS model...\")\n",
    "tts = Vieneu()\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Get model info\n",
    "model_info = TTSModelInfo(tts)\n",
    "model_info.display_capabilities()\n",
    "\n",
    "# Get preset voices\n",
    "print(\"\\nüì¢ Available preset voices:\")\n",
    "preset_voices = model_info.get_preset_voices()\n",
    "for voice in preset_voices:\n",
    "    print(f\"   - {voice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Select or Upload Voice File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload voice file from local machine\nprint(\"üì§ Please upload your voice file (M4A/MP3/WAV)\")\nprint(\"   Select: the-lost-chapter/voices/my-voice.m4a\\n\")\n\nuploaded = files.upload()\n\nif uploaded:\n    uploaded_filename = list(uploaded.keys())[0]\n    selected_file = Path(Directories.UPLOADS) / uploaded_filename\n    \n    with open(selected_file, \"wb\") as f:\n        f.write(uploaded[uploaded_filename])\n    \n    size_mb = len(uploaded[uploaded_filename]) / (1024 * 1024)\n    print(f\"\\n‚úÖ Uploaded: {uploaded_filename}\")\n    print(f\"   Size: {size_mb:.2f} MB\")\n    print(f\"   Saved to: {selected_file}\")\nelse:\n    raise Exception(\"No file uploaded. Please run this cell again and select a file.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Preprocess Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the audio\n",
    "apply_noise_reduction = True  # @param {type:\"boolean\"}\n",
    "\n",
    "audio_data, sample_rate, segments = AudioPreprocessor.process(\n",
    "    str(selected_file),\n",
    "    apply_noise_reduction=apply_noise_reduction\n",
    ")\n",
    "\n",
    "# Save full processed audio\n",
    "processed_full_path = f\"{Directories.PROCESSED}/full_processed.wav\"\n",
    "sf.write(processed_full_path, audio_data, sample_rate)\n",
    "\n",
    "print(\"\\nüîä Full processed audio:\")\n",
    "display(Audio(processed_full_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Preview Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and preview segments\n",
    "print(\"üéµ Extracting candidate segments:\\n\")\n",
    "\n",
    "segment_paths = []\n",
    "\n",
    "for i, segment in enumerate(segments, 1):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìç Segment {i}\")\n",
    "    print(f\"   Time: {segment.start:.1f}s - {segment.end:.1f}s\")\n",
    "    print(f\"   Duration: {segment.duration:.1f}s\")\n",
    "    \n",
    "    segment_audio = SpeechSegmentDetector.extract_segment_audio(\n",
    "        audio_data, sample_rate, segment\n",
    "    )\n",
    "    segment_path = f\"{Directories.PROCESSED}/segment_{i}.wav\"\n",
    "    sf.write(segment_path, segment_audio, sample_rate)\n",
    "    segment_paths.append(segment_path)\n",
    "    \n",
    "    display(Audio(segment_path))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Select Best Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best segment\n",
    "selected_segment_index = 1  # @param {type:\"integer\"}\n",
    "\n",
    "if selected_segment_index < 1 or selected_segment_index > len(segments):\n",
    "    print(f\"‚ö†Ô∏è Invalid selection! Using segment 1\")\n",
    "    selected_segment_index = 1\n",
    "\n",
    "selected_segment_path = segment_paths[selected_segment_index - 1]\n",
    "selected_segment_info = segments[selected_segment_index - 1]\n",
    "\n",
    "print(f\"‚úÖ Selected Segment {selected_segment_index}\")\n",
    "print(f\"   Time: {selected_segment_info.start:.1f}s - {selected_segment_info.end:.1f}s\")\n",
    "print(f\"   Duration: {selected_segment_info.duration:.1f}s\")\n",
    "print(\"\\nüîä Selected audio:\")\n",
    "display(Audio(selected_segment_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Clone Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter transcript for the selected segment\n",
    "sample_transcript = \"Xin ch√†o, ƒë√¢y l√† gi·ªçng n√≥i c·ªßa t√¥i.\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"üìù Transcript: {sample_transcript}\")\n",
    "print(f\"üìÅ Audio file: {selected_segment_path}\")\n",
    "display(Audio(selected_segment_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceCloner:\n",
    "    \"\"\"Handles voice cloning with different API versions\"\"\"\n",
    "    \n",
    "    def __init__(self, tts_model, model_info: TTSModelInfo):\n",
    "        self.tts = tts_model\n",
    "        self.info = model_info\n",
    "    \n",
    "    def clone(self, audio_path: str, transcript: str, voice_name: str = \"MyVoice\"):\n",
    "        \"\"\"\n",
    "        Clone voice using available API method\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to reference audio\n",
    "            transcript: Transcript of reference audio\n",
    "            voice_name: Name for cloned voice\n",
    "        \n",
    "        Returns:\n",
    "            Voice data object or dict for TTS inference\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Cloning voice...\\n\")\n",
    "        \n",
    "        # Try different methods\n",
    "        methods = [\n",
    "            self._try_clone_voice,\n",
    "            self._try_create_voice,\n",
    "            self._try_encode_reference,\n",
    "            self._try_direct_path,\n",
    "            self._try_ref_audio\n",
    "        ]\n",
    "        \n",
    "        for method in methods:\n",
    "            result = method(audio_path, transcript, voice_name)\n",
    "            if result is not None:\n",
    "                return result\n",
    "        \n",
    "        print(\"‚ö†Ô∏è All cloning methods failed. Using preset voice.\")\n",
    "        return self._get_preset_voice()\n",
    "    \n",
    "    def _try_clone_voice(self, audio_path: str, transcript: str, voice_name: str):\n",
    "        \"\"\"Try clone_voice method\"\"\"\n",
    "        if not self.info.has_clone_voice:\n",
    "            return None\n",
    "        try:\n",
    "            print(\"Trying clone_voice method...\")\n",
    "            voice = self.tts.clone_voice(\n",
    "                audio_path=audio_path,\n",
    "                text=transcript,\n",
    "                name=voice_name\n",
    "            )\n",
    "            print(\"‚úÖ clone_voice succeeded!\")\n",
    "            return voice\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå clone_voice failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_create_voice(self, audio_path: str, transcript: str, voice_name: str):\n",
    "        \"\"\"Try create_voice method\"\"\"\n",
    "        if not self.info.has_create_voice:\n",
    "            return None\n",
    "        try:\n",
    "            print(\"Trying create_voice method...\")\n",
    "            voice = self.tts.create_voice(\n",
    "                audio_path=audio_path,\n",
    "                text=transcript\n",
    "            )\n",
    "            print(\"‚úÖ create_voice succeeded!\")\n",
    "            return voice\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå create_voice failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_encode_reference(self, audio_path: str, transcript: str, voice_name: str):\n",
    "        \"\"\"Try encode_reference method\"\"\"\n",
    "        if not self.info.has_encode_reference:\n",
    "            return None\n",
    "        try:\n",
    "            print(\"Trying encode_reference method...\")\n",
    "            voice = self.tts.encode_reference(audio_path)\n",
    "            print(\"‚úÖ encode_reference succeeded!\")\n",
    "            return voice\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå encode_reference failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_direct_path(self, audio_path: str, transcript: str, voice_name: str):\n",
    "        \"\"\"Try using path directly\"\"\"\n",
    "        try:\n",
    "            print(\"Testing direct path method...\")\n",
    "            self.tts.infer(text=\"Test\", voice=audio_path)\n",
    "            print(\"‚úÖ Direct path works!\")\n",
    "            return audio_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Direct path failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_ref_audio(self, audio_path: str, transcript: str, voice_name: str):\n",
    "        \"\"\"Try ref_audio parameter\"\"\"\n",
    "        try:\n",
    "            print(\"Testing ref_audio parameter...\")\n",
    "            self.tts.infer(\n",
    "                text=\"Test\",\n",
    "                ref_audio=audio_path,\n",
    "                ref_text=transcript\n",
    "            )\n",
    "            print(\"‚úÖ ref_audio parameter works!\")\n",
    "            return {\"ref_audio\": audio_path, \"ref_text\": transcript}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ref_audio failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _get_preset_voice(self):\n",
    "        \"\"\"Get first available preset voice\"\"\"\n",
    "        voices = self.info.get_preset_voices()\n",
    "        if voices:\n",
    "            voice_name = voices[0]\n",
    "            print(f\"Using preset voice: {voice_name}\")\n",
    "            return self.tts.get_preset_voice(voice_name)\n",
    "        return None\n",
    "\n",
    "# Clone the voice\n",
    "cloner = VoiceCloner(tts, model_info)\n",
    "cloned_voice = cloner.clone(selected_segment_path, sample_transcript)\n",
    "\n",
    "print(f\"\\n‚úÖ Voice setup complete!\")\n",
    "print(f\"   Voice type: {type(cloned_voice)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Generate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechGenerator:\n",
    "    \"\"\"Generates speech from text using cloned voice\"\"\"\n",
    "    \n",
    "    def __init__(self, tts_model):\n",
    "        self.tts = tts_model\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        voice_data,\n",
    "        output_path: str,\n",
    "        sample_rate: int = None\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Generate speech audio file\n",
    "        \n",
    "        Args:\n",
    "            text: Text to synthesize\n",
    "            voice_data: Cloned voice data\n",
    "            output_path: Path to save audio\n",
    "            sample_rate: Output sample rate (uses config default if None)\n",
    "        \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if sample_rate is None:\n",
    "            sample_rate = AudioConfig.TARGET_SAMPLE_RATE\n",
    "        \n",
    "        # Try different inference methods\n",
    "        audio = self._try_inference(text, voice_data)\n",
    "        \n",
    "        if audio is not None:\n",
    "            sf.write(output_path, audio, sample_rate)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _try_inference(self, text: str, voice_data):\n",
    "        \"\"\"Try different inference methods\"\"\"\n",
    "        methods = [\n",
    "            lambda: self._infer_with_voice(text, voice_data),\n",
    "            lambda: self._infer_with_ref_audio(text, voice_data),\n",
    "            lambda: self._infer_with_speaker_wav(text, voice_data),\n",
    "            lambda: self._infer_simple(text, voice_data)\n",
    "        ]\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                return method()\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _infer_with_voice(self, text: str, voice_data):\n",
    "        \"\"\"Standard inference with voice parameter\"\"\"\n",
    "        if isinstance(voice_data, dict) and \"ref_audio\" in voice_data:\n",
    "            return self.tts.infer(\n",
    "                text=text,\n",
    "                ref_audio=voice_data[\"ref_audio\"],\n",
    "                ref_text=voice_data[\"ref_text\"]\n",
    "            )\n",
    "        else:\n",
    "            return self.tts.infer(\n",
    "                text=text,\n",
    "                voice=voice_data,\n",
    "                temperature=1.0,\n",
    "                top_k=50\n",
    "            )\n",
    "    \n",
    "    def _infer_with_ref_audio(self, text: str, voice_data):\n",
    "        \"\"\"Inference with ref_audio parameter\"\"\"\n",
    "        if isinstance(voice_data, dict):\n",
    "            return self.tts.infer(\n",
    "                text=text,\n",
    "                ref_audio=voice_data[\"ref_audio\"],\n",
    "                ref_text=voice_data[\"ref_text\"]\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def _infer_with_speaker_wav(self, text: str, voice_data):\n",
    "        \"\"\"Inference with speaker_wav (XTTS-style)\"\"\"\n",
    "        if isinstance(voice_data, str):\n",
    "            return self.tts.infer(text=text, speaker_wav=voice_data)\n",
    "        return None\n",
    "    \n",
    "    def _infer_simple(self, text: str, voice_data):\n",
    "        \"\"\"Simple inference without extra params\"\"\"\n",
    "        return self.tts.infer(text=text, voice=voice_data)\n",
    "\n",
    "# Create generator\n",
    "generator = SpeechGenerator(tts)\n",
    "print(\"‚úÖ Speech generator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate speech with cloned voice\n",
    "text_to_speak = \"Xin ch√†o m·ªçi ng∆∞·ªùi. T√¥i l√† tr·ª£ l√Ω ·∫£o ƒë∆∞·ª£c t·∫°o b·ªüi VieNeu TTS. R·∫•t vui ƒë∆∞·ª£c g·∫∑p c√°c b·∫°n.\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"üìù Text: {text_to_speak}\")\n",
    "print(\"\\nüîÑ Generating speech...\")\n",
    "\n",
    "output_path = f\"{Directories.OUTPUTS}/cloned_speech.wav\"\n",
    "\n",
    "if generator.generate(text_to_speak, cloned_voice, output_path):\n",
    "    print(\"\\n‚úÖ Speech generated!\")\n",
    "    print(\"\\nüîä Your cloned voice:\")\n",
    "    display(Audio(output_path))\n",
    "else:\n",
    "    print(\"\\n‚ùå Generation failed\")\n",
    "    print(\"\\nTrying with default voice...\")\n",
    "    try:\n",
    "        audio = tts.infer(text=text_to_speak)\n",
    "        sf.write(output_path, audio, AudioConfig.TARGET_SAMPLE_RATE)\n",
    "        print(\"\\nüîä Generated with default voice:\")\n",
    "        display(Audio(output_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Default voice also failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple samples\n",
    "example_texts = [\n",
    "    \"H√¥m nay th·ªùi ti·∫øt r·∫•t ƒë·∫πp, ch√∫ng ta ƒëi d·∫°o c√¥ng vi√™n nh√©.\",\n",
    "    \"Ch√†o bu·ªïi s√°ng! B·∫°n ƒë√£ ƒÉn s√°ng ch∆∞a?\",\n",
    "    \"T√¥i c√≥ th·ªÉ gi√∫p b·∫°n ƒë·ªçc s√°ch, t·∫°o chatbot, ho·∫∑c l√†m tr·ª£ l√Ω ·∫£o.\",\n",
    "    \"Hello! I can also speak English with Vietnamese accent.\"\n",
    "]\n",
    "\n",
    "print(\"üéµ Generating samples...\\n\")\n",
    "\n",
    "for i, text in enumerate(example_texts, 1):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìç Sample {i}: {text}\")\n",
    "    \n",
    "    output_path = f\"{Directories.OUTPUTS}/sample_{i}.wav\"\n",
    "    if generator.generate(text, cloned_voice, output_path):\n",
    "        display(Audio(output_path))\n",
    "    else:\n",
    "        print(\"‚ùå Failed to generate\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Interactive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive TTS - run this cell multiple times with different text\n",
    "your_text = \"Nh·∫≠p vƒÉn b·∫£n c·ªßa b·∫°n v√†o ƒë√¢y!\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"üîÑ Generating: {your_text}\")\n",
    "output_path = f\"{Directories.OUTPUTS}/interactive.wav\"\n",
    "\n",
    "if generator.generate(your_text, cloned_voice, output_path):\n",
    "    print(\"\\nüîä Result:\")\n",
    "    display(Audio(output_path))\n",
    "else:\n",
    "    print(\"‚ùå Generation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Create ZIP with all outputs\n",
    "zip_path = \"vieneu_outputs.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for file in Path(Directories.OUTPUTS).glob(\"*.wav\"):\n",
    "        zipf.write(file, f\"outputs/{file.name}\")\n",
    "    for file in Path(Directories.PROCESSED).glob(\"*.wav\"):\n",
    "        zipf.write(file, f\"processed/{file.name}\")\n",
    "\n",
    "print(\"üì¶ Files in ZIP:\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "    for name in zipf.namelist():\n",
    "        print(f\"   - {name}\")\n",
    "\n",
    "print(\"\\nüì• Downloading...\")\n",
    "files.download(zip_path)\n",
    "print(\"‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "tts.close()\n",
    "print(\"‚úÖ Resources cleaned up!\")\n",
    "print(\"\\nüéâ Voice cloning session complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}